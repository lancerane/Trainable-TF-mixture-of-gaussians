{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainable mixture of gaussians model where base distributions all have diagonal covariance matrix. Based on OpenAI's Baselines implementation of probability density under a multivariate gaussian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base gaussian implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagGaussianPd():\n",
    "    def __init__(self, flat):\n",
    "        self.flat = flat\n",
    "        mean, logstd = tf.split(axis=len(flat.shape)-1, num_or_size_splits=2, value=flat)\n",
    "        self.mean = mean\n",
    "        self.logstd = logstd\n",
    "        self.std = tf.exp(logstd)\n",
    "    def flatparam(self):\n",
    "        return self.flat\n",
    "    def mode(self):\n",
    "        return self.mean\n",
    "    def neglogp(self, x):\n",
    "        return 0.5 * tf.reduce_sum(tf.square((x - self.mean) / self.std), axis=-1) \\\n",
    "               + 0.5 * np.log(2.0 * np.pi) * tf.to_float(tf.shape(x)[-1]) \\\n",
    "               + tf.reduce_sum(self.logstd, axis=-1)\n",
    "    def sample(self):\n",
    "        return self.mean + self.std * tf.random_normal(tf.shape(self.mean))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixture model builds on this. It's initialised with two arguments: a list of gaussian models and a vector of mixing coefficients, defining how the base gaussians are weighted in the mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixOfGaussianPd():\n",
    "    def __init__(self, list_of_gaussians, coeffs): \n",
    "        self.list_of_gaussians = list_of_gaussians\n",
    "        self.K = len(list_of_gaussians)\n",
    "        self.coeffs = coeffs\n",
    "        self.mean = tf.matmul(self.coeffs, [i.mean[0] for i in self.list_of_gaussians])\n",
    "    def mode(self):\n",
    "        ind = np.argmax(self.coeffs.eval()[0])\n",
    "        return self.list_of_gaussians[ind].mean\n",
    "    def neglogp(self, x):\n",
    "        individual_ps = [tf.exp(-i.neglogp(x)) for i in self.list_of_gaussians]    \n",
    "        return -tf.log(tf.matmul(self.coeffs, individual_ps))\n",
    "    def sample(self):\n",
    "        ind = np.random.choice(self.K, 1, p=self.coeffs.eval()[0])[0]\n",
    "        return self.list_of_gaussians[ind].sample()\n",
    "    def logp(self, x):\n",
    "        return - self.neglogp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of components is chosen a priori: here it's 2, arbitrarily.  Each component is described by a dictionary defining a multivariate normal distribution, with an entry for each variable. Each entry lists the corresponding mean and log standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm1 = {'x1':[0.2, np.log(0.05)], 'x2':[0.1,np.log(0.05)], 'x3':[0.9,np.log(0.02)]}\n",
    "norm2 = {'x1':[0.3, np.log(0.05)], 'x2':[0.5,np.log(0.05)], 'x3':[0.7,np.log(0.03)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pack the dictionaries into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_normal_params = [norm1, norm2]\n",
    "num_components = len(list_of_normal_params)\n",
    "num_vars = len(list_of_normal_params[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the TF variables for the means and standard deviations of each component of the model and collect into a list to be used for initialising the individual gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = []\n",
    "\n",
    "for k in range(num_components):\n",
    "    mean =tf.get_variable(name=\"mean_%i\"%k, shape=[1,num_vars], \n",
    "                          initializer=tf.constant_initializer(value=[i[0] for i in list_of_normal_params[k] \\\n",
    "                                                                     .values()]), trainable=True)\n",
    "    logstd = tf.get_variable(name=\"logstd_%i\"%k, shape=[1,num_vars], \n",
    "                             initializer=tf.constant_initializer(value=[i[1] for i in list_of_normal_params[k] \\\n",
    "                                                                        .values()]), trainable=True)\n",
    "    param = tf.concat([mean, mean *0.0 +logstd], axis=1)\n",
    "    param_list.append(param)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need mixing coefficients corresponding to a prior probability on each of the components. Initialise these to a uniform distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_coeffs = tf.nn.softmax(tf.get_variable(name=\"mix_coeffs\", shape=[1,num_components],\n",
    "                                           initializer=tf.constant_initializer(\n",
    "                                               value=[1/num_components for i in range(num_components)]), \n",
    "                                           trainable=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a list of multivariate normals where each is initialised from a flat vector of parameters, created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_gaussians = [DiagGaussianPd(i) for i in param_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the mixture model from this list of base distributions, and the mixture coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd = MixOfGaussianPd(list_of_gaussians, mix_coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainable parameters are the vectors of means and log standard deviations, and the mixing coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'mean_0:0' shape=(1, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'logstd_0:0' shape=(1, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'mean_1:0' shape=(1, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'logstd_1:0' shape=(1, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'mix_coeffs:0' shape=(1, 2) dtype=float32_ref>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model implements sampling and log probability. It can be trained in Tensorflow by defining a loss that includes the pd.logp() function, and computing derivatives with respect to its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess= tf.InteractiveSession()\n",
    "tf.get_default_session().run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.30372337,  0.63920331,  0.73118794]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.sample().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-63.50749207]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.logp([0.7, 0.1, 0.6]).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25      ,  0.30000001,  0.79999995]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.mean.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
